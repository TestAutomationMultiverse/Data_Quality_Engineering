{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc10c2a-7e3d-46af-987c-c53d5aaacfe5",
   "metadata": {},
   "source": [
    "# PySpark Integration Tests with Chispa\n",
    "## Introduction\n",
    "Integration testing is a vital aspect of software development. It guarantees the quality and reliability of software modules. Therefore, data engineers must learn how to create effective integration tests.\n",
    "\n",
    "**So, What are Integration Tests?**\n",
    "\n",
    "Integration tests are the second level of software testing, where individual software components are tested to ensure they work together as intended. Integration testing aims to identify issues with component interaction, validate data flow, and provide the proper behaviour of software when different parts are combined.\n",
    "\n",
    "**And Why are Integration Tests Important?**\n",
    "\n",
    "Integration tests help verify that individual components or modules of a software system work as they should together. They ensure that the various software modules function smoothly.  \n",
    "There may be problems with hardware compatibility, system behaviour and verifying third-party APIs or tools, so we need integration testing to confirm that the data these produce and accept is correct.\n",
    "https://github.com/MrPowers/chispa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a098fc83-6e9f-4bb8-9e4b-9dd1143634d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/16 11:03:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.eventLog.dir', 'hdfs:///var/log/spark'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1734346740798_0001'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.app.submitTime', '1734347028540'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.yarn.jars', 'hdfs://master/spark/jars/*.jar'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///var/log/spark'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da73a876-5d5d-49bd-969a-b92dda9b7b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 11:24:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/12/16 11:24:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------+------------+-----------+\n",
      "|   Name|Age|        City|     Country| Department|\n",
      "+-------+---+------------+------------+-----------+\n",
      "|   John| 28|    New York|         USA|      Sales|\n",
      "|  Alice| 32| Los Angeles|         USA|  Marketing|\n",
      "|    Bob| 24|     Chicago|         USA|Engineering|\n",
      "|   Sara| 29|     Toronto|      Canada|    Finance|\n",
      "|  David| 30|      London|          UK|         HR|\n",
      "|  Emily| 27|      Sydney|   Australia|      Sales|\n",
      "| Daniel| 35|       Paris|      France|  Marketing|\n",
      "|   Ella| 26|      Berlin|     Germany|Engineering|\n",
      "|  Grace| 31|      Madrid|       Spain|    Finance|\n",
      "|William| 33|        Rome|       Italy|         HR|\n",
      "| Olivia| 25|       Tokyo|       Japan|      Sales|\n",
      "|   Liam| 29|     Beijing|       China|  Marketing|\n",
      "| Sophia| 34|      Mumbai|       India|Engineering|\n",
      "|  Aiden| 28|   Cape Town|South Africa|    Finance|\n",
      "|    Mia| 31|Buenos Aires|   Argentina|         HR|\n",
      "+-------+---+------------+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|Concatenated_Column                    |\n",
      "+---------------------------------------+\n",
      "|John_28_New York_USA_Sales             |\n",
      "|Alice_32_Los Angeles_USA_Marketing     |\n",
      "|Bob_24_Chicago_USA_Engineering         |\n",
      "|Sara_29_Toronto_Canada_Finance         |\n",
      "|David_30_London_UK_HR                  |\n",
      "|Emily_27_Sydney_Australia_Sales        |\n",
      "|Daniel_35_Paris_France_Marketing       |\n",
      "|Ella_26_Berlin_Germany_Engineering     |\n",
      "|Grace_31_Madrid_Spain_Finance          |\n",
      "|William_33_Rome_Italy_HR               |\n",
      "|Olivia_25_Tokyo_Japan_Sales            |\n",
      "|Liam_29_Beijing_China_Marketing        |\n",
      "|Sophia_34_Mumbai_India_Engineering     |\n",
      "|Aiden_28_Cape Town_South Africa_Finance|\n",
      "|Mia_31_Buenos Aires_Argentina_HR       |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Call the test function\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[43mtest_get_concatenated_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m, in \u001b[0;36mtest_get_concatenated_column\u001b[0;34m(schema, data)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_get_concatenated_column\u001b[39m(schema, data):\n\u001b[0;32m---> 62\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     actual_output \u001b[38;5;241m=\u001b[39m get_concatenated_column(table)\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m     expected_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn_28_New_York_USA_Sales\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Corrected expected output\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mcreate_table\u001b[0;34m(spark, schema, table_data)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_table\u001b[39m(spark, schema, table_data):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Creates a Spark DataFrame from given schema and data.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m        Spark DataFrame.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1276\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1275\u001b[0m     )\n\u001b[0;32m-> 1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1318\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1318\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:979\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[1;32m    978\u001b[0m internal_data \u001b[38;5;241m=\u001b[39m [struct\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[0;32m--> 979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_data\u001b[49m\u001b[43m)\u001b[49m, struct\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py:780\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallelize\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RDD[T]:\n\u001b[1;32m    749\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;124;03m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 780\u001b[0m     numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n\u001b[1;32m    782\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(c)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py:627\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    617\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import concat_ws, col\n",
    "from chispa.dataframe_comparer import *\n",
    "import argparse\n",
    "\n",
    "spark = SparkSession.builder.appName(\"chispa_test\").getOrCreate()\n",
    "\n",
    "table_values = [(\"John\", 28, \"New York\", \"USA\", \"Sales\"),\n",
    "                (\"Alice\", 32, \"Los Angeles\", \"USA\", \"Marketing\"),\n",
    "                (\"Bob\", 24, \"Chicago\", \"USA\", \"Engineering\"),\n",
    "                (\"Sara\", 29, \"Toronto\", \"Canada\", \"Finance\"),\n",
    "                (\"David\", 30, \"London\", \"UK\", \"HR\"),\n",
    "                (\"Emily\", 27, \"Sydney\", \"Australia\", \"Sales\"),\n",
    "                (\"Daniel\", 35, \"Paris\", \"France\", \"Marketing\"),\n",
    "                (\"Ella\", 26, \"Berlin\", \"Germany\", \"Engineering\"),\n",
    "                (\"Grace\", 31, \"Madrid\", \"Spain\", \"Finance\"),\n",
    "                (\"William\", 33, \"Rome\", \"Italy\", \"HR\"),\n",
    "                (\"Olivia\", 25, \"Tokyo\", \"Japan\", \"Sales\"),\n",
    "                (\"Liam\", 29, \"Beijing\", \"China\", \"Marketing\"),\n",
    "                (\"Sophia\", 34, \"Mumbai\", \"India\", \"Engineering\"),\n",
    "                (\"Aiden\", 28, \"Cape Town\", \"South Africa\", \"Finance\"),\n",
    "                (\"Mia\", 31, \"Buenos Aires\", \"Argentina\", \"HR\")]\n",
    "\n",
    "schema_sample = [\n",
    "    \"Name\", \"Age\", \"City\", \"Country\", \"Department\"\n",
    "]\n",
    "\n",
    "def create_table(spark, schema, table_data):\n",
    "    \"\"\"\n",
    "    Creates a Spark DataFrame from given schema and data.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession object.\n",
    "        schema: List of column names.\n",
    "        table_data: List of tuples, where each tuple represents a row of data.\n",
    "\n",
    "    Returns:\n",
    "        Spark DataFrame.\n",
    "    \"\"\"\n",
    "    df = spark.createDataFrame(table_data, schema)\n",
    "    df.show()\n",
    "    return df\n",
    "\n",
    "def get_concatenated_column(spark_table):\n",
    "    \"\"\"\n",
    "    Concatenates all columns of the given Spark DataFrame into a single column.\n",
    "\n",
    "    Args:\n",
    "        spark_table: Spark DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Spark DataFrame with a single column containing the concatenated values.\n",
    "    \"\"\"\n",
    "    column_names = spark_table.columns\n",
    "    concatenated_column = spark_table.select(\n",
    "        concat_ws(\"_\", *[col(c) for c in column_names]).alias(\"Concatenated_Column\")\n",
    "    )\n",
    "    concatenated_column.show(truncate=False)\n",
    "    return concatenated_column\n",
    "\n",
    "def test_get_concatenated_column(schema, data):\n",
    "    table = create_table(spark, schema, data)\n",
    "    actual_output = get_concatenated_column(table).collect()[0][0]\n",
    "    expected_output = \"John_28_New_York_USA_Sales\"  # Corrected expected output\n",
    "\n",
    "    assert actual_output == expected_output, f\"Expected: {expected_output}, Actual: {actual_output}\"\n",
    "\n",
    "    schema = [StructField(\"Concatenated_Column\", StringType(), True)] \n",
    "    expected_df = spark.createDataFrame([Row(Concatenated_Column=expected_output)], schema)\n",
    "    actual_df = spark.createDataFrame([Row(Concatenated_Column=actual_output)], schema)\n",
    "    assert_df_equality(expected_df, actual_df)\n",
    "    print(\"all's good\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ColumnConcatenation\").getOrCreate()\n",
    "table = create_table(spark, schema_sample, table_values)\n",
    "get_concatenated_column(table)\n",
    "spark.stop()\n",
    "\n",
    "# Call the test function\n",
    "test_get_concatenated_column(schema_sample, table_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca8871-67a2-4455-b976-c492fea31474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc095d03-0c36-43f0-97a8-a05476304a45",
   "metadata": {},
   "source": [
    "## Unit Tests for PySpark Applications Using unittest and pytest Libraries\n",
    "\n",
    "TL;DR: Software testing, and in particular, unit testing, is a crucial step in modern Data Engineering. Pytest and unittest are great tools for developing unit tests for PySpark applications. In this article, I provide code examples using both libraries. Also, I discuss the advantages and disadvantages that each of them brings. The choice depends on your needs and previous experience.\n",
    "\n",
    "\n",
    "**1\\. Introduction**\n",
    "\n",
    "**2\\. What is Testing in Software Development?**\n",
    "\n",
    "**2.1 What are the Test Types?**\n",
    "\n",
    "**2.2 Understanding Unit Testing and Its Importance**\n",
    "\n",
    "**2.3 What are the Purposes of the unittest and pytest Libraries?**\n",
    "\n",
    "**2.4 How Can I Execute These Tests?**\n",
    "\n",
    "**2.5 Summary and Comparison**\n",
    "\n",
    "**3\\. Conclusion**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4812907a-39c1-4bc1-a37c-c1ebf2d35455",
   "metadata": {},
   "source": [
    "# 1\\. Introduction\n",
    "\n",
    "In this article, I will talk about how we can create unit tests for PySpark applications using Python unittest and pytest libraries. Before jumping into the topic, it makes sense to talk briefly about what testing is in Software Development, what types of tests exist, what unit testing is and why it is even needed?\n",
    "\n",
    "# 2\\. What is Testing in Software Development?\n",
    "\n",
    "Testing in software development refers to the process of evaluating and verifying that a software program or application works as intended. It’s a crucial step in the software development life cycle, ensuring that the software meets specified requirements and is free of defects or bugs that could adversely impact its performance, reliability or security.\n",
    "\n",
    "In this article, I won’t go into details of the benefits of testing and Test Driven Development (TDD). But as a data engineer/data scientist/ML engineer (i.e., data person) you should care about the quality of the software you are building. However, I will still briefly talk about the importance of unit testing below.\n",
    "\n",
    "# 2.1 What are the Test Types?\n",
    "\n",
    "There are numerous types of tests in software development, each designed for a specific purpose and stage in the development process. The most commonly implemented test types are \\[1\\]:\n",
    "\n",
    "-   **Unit Testing**: These are the smallest tests which validate that an individual component (e.g., function) of the software works as intended. That’s why it is expected to run the unit tests in the Continuous Integration (CI) pipeline such as after each commit or when a PR is opened depending on your DevOps workflow.\n",
    "-   **Integration Testing**: These tests check if multiple components work well together. The number of integration tests is less than unit tests but higher than E2E tests. They also take longer to execute than unit tests but take less time than E2E tests. Integration tests can also be included in the CI pipeline (e.g., when PR opened into develop and/or master). Since the decision of when to run which test and which branch strategy to use are DevOps-related topics, I won’t dive into this topic for now.\n",
    "-   **End-to-End Testing**: Checks the application flow from beginning to end making sure that everything works well. This is very costly and most of the time done manually (especially at the beginning of the development) in the data engineering world. It is also possible to automate E2E tests but the execution time will still be much longer than other types of tests. Be aware that, in the Web and Mobile Development contexts E2E testing is different where the goal is to simulate real user actions using various tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709b14a-9f5e-4a9b-9433-99d08516ef35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
