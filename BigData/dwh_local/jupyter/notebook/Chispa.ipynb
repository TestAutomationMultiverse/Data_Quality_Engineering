{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc10c2a-7e3d-46af-987c-c53d5aaacfe5",
   "metadata": {},
   "source": [
    "# PySpark Integration Tests with Chispa\n",
    "## Introduction\n",
    "Integration testing is a vital aspect of software development. It guarantees the quality and reliability of software modules. Therefore, data engineers must learn how to create effective integration tests.\n",
    "\n",
    "**So, What are Integration Tests?**\n",
    "\n",
    "Integration tests are the second level of software testing, where individual software components are tested to ensure they work together as intended. Integration testing aims to identify issues with component interaction, validate data flow, and provide the proper behaviour of software when different parts are combined.\n",
    "\n",
    "**And Why are Integration Tests Important?**\n",
    "\n",
    "Integration tests help verify that individual components or modules of a software system work as they should together. They ensure that the various software modules function smoothly.  \n",
    "There may be problems with hardware compatibility, system behaviour and verifying third-party APIs or tools, so we need integration testing to confirm that the data these produce and accept is correct.\n",
    "https://github.com/MrPowers/chispa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a098fc83-6e9f-4bb8-9e4b-9dd1143634d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/10 08:35:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.eventLog.dir', 'hdfs:///var/log/spark'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.app.submitTime', '1733819731054'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1733817654557_0001'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.yarn.jars', 'hdfs://master/spark/jars/*.jar'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///var/log/spark'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da73a876-5d5d-49bd-969a-b92dda9b7b6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chispa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession, Row\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat_ws, col\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchispa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe_comparer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchispa_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chispa'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import concat_ws, col\n",
    "from chispa.dataframe_comparer import *\n",
    "import argparse\n",
    "\n",
    "spark = SparkSession.builder.appName(\"chispa_test\").getOrCreate()\n",
    "\n",
    "table_values = [(\"John\", 28, \"New York\", \"USA\", \"Sales\"),\n",
    "                (\"Alice\", 32, \"Los Angeles\", \"USA\", \"Marketing\"),\n",
    "                (\"Bob\", 24, \"Chicago\", \"USA\", \"Engineering\"),\n",
    "                (\"Sara\", 29, \"Toronto\", \"Canada\", \"Finance\"),\n",
    "                (\"David\", 30, \"London\", \"UK\", \"HR\"),\n",
    "                (\"Emily\", 27, \"Sydney\", \"Australia\", \"Sales\"),\n",
    "                (\"Daniel\", 35, \"Paris\", \"France\", \"Marketing\"),\n",
    "                (\"Ella\", 26, \"Berlin\", \"Germany\", \"Engineering\"),\n",
    "                (\"Grace\", 31, \"Madrid\", \"Spain\", \"Finance\"),\n",
    "                (\"William\", 33, \"Rome\", \"Italy\", \"HR\"),\n",
    "                (\"Olivia\", 25, \"Tokyo\", \"Japan\", \"Sales\"),\n",
    "                (\"Liam\", 29, \"Beijing\", \"China\", \"Marketing\"),\n",
    "                (\"Sophia\", 34, \"Mumbai\", \"India\", \"Engineering\"),\n",
    "                (\"Aiden\", 28, \"Cape Town\", \"South Africa\", \"Finance\"),\n",
    "                (\"Mia\", 31, \"Buenos Aires\", \"Argentina\", \"HR\")]\n",
    "\n",
    "schema_sample = [\n",
    "    \"Name\", \"Age\", \"City\", \"Country\", \"Department\"\n",
    "]\n",
    "\n",
    "def create_table(spark, schema, table_data):\n",
    "    \"\"\"\n",
    "    Creates a Spark DataFrame from given schema and data.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession object.\n",
    "        schema: List of column names.\n",
    "        table_data: List of tuples, where each tuple represents a row of data.\n",
    "\n",
    "    Returns:\n",
    "        Spark DataFrame.\n",
    "    \"\"\"\n",
    "    df = spark.createDataFrame(table_data, schema)\n",
    "    df.show()\n",
    "    return df\n",
    "\n",
    "def get_concatenated_column(spark_table):\n",
    "    \"\"\"\n",
    "    Concatenates all columns of the given Spark DataFrame into a single column.\n",
    "\n",
    "    Args:\n",
    "        spark_table: Spark DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Spark DataFrame with a single column containing the concatenated values.\n",
    "    \"\"\"\n",
    "    column_names = spark_table.columns\n",
    "    concatenated_column = spark_table.select(\n",
    "        concat_ws(\"_\", *[col(c) for c in column_names]).alias(\"Concatenated_Column\")\n",
    "    )\n",
    "    concatenated_column.show(truncate=False)\n",
    "    return concatenated_column\n",
    "\n",
    "def test_get_concatenated_column(schema, data):\n",
    "    table = create_table(spark, schema, data)\n",
    "    actual_output = get_concatenated_column(table).collect()[0][0]\n",
    "    expected_output = \"John_28_New_York_USA_Sales\"  # Corrected expected output\n",
    "\n",
    "    assert actual_output == expected_output, f\"Expected: {expected_output}, Actual: {actual_output}\"\n",
    "\n",
    "    schema = [StructField(\"Concatenated_Column\", StringType(), True)] \n",
    "    expected_df = spark.createDataFrame([Row(Concatenated_Column=expected_output)], schema)\n",
    "    actual_df = spark.createDataFrame([Row(Concatenated_Column=actual_output)], schema)\n",
    "    assert_df_equality(expected_df, actual_df)\n",
    "    print(\"all's good\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ColumnConcatenation\").getOrCreate()\n",
    "table = create_table(spark, schema_sample, table_values)\n",
    "get_concatenated_column(table)\n",
    "spark.stop()\n",
    "\n",
    "# Call the test function\n",
    "test_get_concatenated_column(schema_sample, table_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca8871-67a2-4455-b976-c492fea31474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc095d03-0c36-43f0-97a8-a05476304a45",
   "metadata": {},
   "source": [
    "## Unit Tests for PySpark Applications Using unittest and pytest Libraries\n",
    "\n",
    "TL;DR: Software testing, and in particular, unit testing, is a crucial step in modern Data Engineering. Pytest and unittest are great tools for developing unit tests for PySpark applications. In this article, I provide code examples using both libraries. Also, I discuss the advantages and disadvantages that each of them brings. The choice depends on your needs and previous experience.\n",
    "\n",
    "\n",
    "**1\\. Introduction**\n",
    "\n",
    "**2\\. What is Testing in Software Development?**\n",
    "\n",
    "**2.1 What are the Test Types?**\n",
    "\n",
    "**2.2 Understanding Unit Testing and Its Importance**\n",
    "\n",
    "**2.3 What are the Purposes of the unittest and pytest Libraries?**\n",
    "\n",
    "**2.4 How Can I Execute These Tests?**\n",
    "\n",
    "**2.5 Summary and Comparison**\n",
    "\n",
    "**3\\. Conclusion**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105984b-f751-445d-8e30-d8e56fd8f3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
